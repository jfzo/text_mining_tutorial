{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cursillo de minería de texto\n",
    "## Centro Interdisciplinario para la Innovación en Salud\n",
    "### Juan Zamora Osorio - Abril, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte I - Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de características\n",
    "Consideremos el siguiente (mini) texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "txt_example = \"\"\"Enron Corporation (former NYSE ticker symbol ENE) was an\n",
    "American energy company based in Houston, Texas. Before its bankruptcy\n",
    "in late 2001, Enron employed approximately 22,000[1] and was one of\n",
    "the world’s leading electricity, natural gas, pulp and paper, and\n",
    "communications companies, with claimed revenues of nearly $101 billion\n",
    "in 2000.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al aplicar la función `word_tokenize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Enron', 'Corporation', '(', 'former', 'NYSE', 'ticker', 'symbol', 'ENE', ')', 'was', 'an', 'American', 'energy', 'company', 'based', 'in', 'Houston', ',', 'Texas', '.', 'Before', 'its', 'bankruptcy', 'in', 'late', '2001', ',', 'Enron', 'employed', 'approximately', '22,000', '[', '1', ']', 'and', 'was', 'one', 'of', 'the', 'world\\xe2\\x80\\x99s', 'leading', 'electricity', ',', 'natural', 'gas', ',', 'pulp', 'and', 'paper', ',', 'and', 'communications', 'companies', ',', 'with', 'claimed', 'revenues', 'of', 'nearly', '$', '101', 'billion', 'in', '2000', '.']\n"
     ]
    }
   ],
   "source": [
    "print nltk.word_tokenize(txt_example, language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción consiste en usar [expresiones regulares](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Enron', 'Corporation', '(', 'former', 'NYSE', 'ticker', 'symbol', 'ENE', ')', 'was', 'an', 'American', 'energy', 'company', 'based', 'in', 'Houston', ',', 'Texas', '.', 'Before', 'its', 'bankruptcy', 'in', 'late', '2001', ',', 'Enron', 'employed', 'approximately', '22,000', '[', '1', ']', 'and', 'was', 'one', 'of', 'the', 'world\\xe2', 's', 'leading', 'electricity', ',', 'natural', 'gas', ',', 'pulp', 'and', 'paper', ',', 'and', 'communications', 'companies', ',', 'with', 'claimed', 'revenues', 'of', 'nearly', '$101', 'billion', 'in', '2000', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pat = r\"\"\"(?ux)\n",
    "(?:[^\\W\\d_]\\.)+                   # Abreviaciones\n",
    "| [^\\W\\d_]+(?:-[^\\W\\d_])*(?:'s)?  # Palabras con guión (compuestas)\n",
    "| \\d{4}                           # Año\n",
    "| \\d{1,3}(?:,\\d{3})*              # Numero\n",
    "| \\$\\d+(?:\\.\\d{2})?               # Moneda\n",
    "| \\d{1,3}(?:\\.\\d+)?\\s%            # Porcentaje\n",
    "| \\.\\.\\.                          # Puntos suspensivos (elipsis)\n",
    "| [.,:\"'?!():-__`/]               # Otros simbolos\n",
    "\"\"\"\n",
    "\n",
    "print nltk.regexp_tokenize(txt_example, pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorías de términos y reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización (Stemming):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defines --> defin\n",
      "define --> defin\n",
      "defining --> defin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print \"defines -->\",stemmer.stem(\"defines\")\n",
    "print \"define -->\",stemmer.stem(\"define\")\n",
    "print \"defining -->\",stemmer.stem(\"defining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta:__ ¿Como lematizar todos los términos de la lista `items`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging:\n",
    "\n",
    "NN noun, VB verb, JJ adjective, RB adverb ...(revisar salida de la instrucción `nltk.help.upenn_tagset()`)\n",
    "\n",
    "Se encontrará más detalle en el [libro en línea del libro de _NLTK_](http://www.nltk.org/book/ch05.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Enron', 'NNP'), ('Corporation', 'NNP'), ('(', '('), ('former', 'JJ'), ('NYSE', 'NNP'), ('ticker', 'NN'), ('symbol', 'NN'), ('ENE', 'NNP'), (')', ')'), ('was', 'VBD'), ('an', 'DT'), ('American', 'JJ'), ('energy', 'NN'), ('company', 'NN'), ('based', 'VBN'), ('in', 'IN'), ('Houston', 'NNP'), (',', ','), ('Texas', 'NNP'), ('.', '.'), ('Before', 'IN'), ('its', 'PRP$'), ('bankruptcy', 'NN'), ('in', 'IN'), ('late', 'JJ'), ('2001', 'CD'), (',', ','), ('Enron', 'NNP'), ('employed', 'VBD'), ('approximately', 'RB'), ('22,000', 'CD'), ('[', 'JJ'), ('1', 'CD'), (']', 'NN'), ('and', 'CC'), ('was', 'VBD'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('world\\xe2\\x80\\x99s', 'NN'), ('leading', 'VBG'), ('electricity', 'NN'), (',', ','), ('natural', 'JJ'), ('gas', 'NN'), (',', ','), ('pulp', 'NN'), ('and', 'CC'), ('paper', 'NN'), (',', ','), ('and', 'CC'), ('communications', 'NNS'), ('companies', 'NNS'), (',', ','), ('with', 'IN'), ('claimed', 'JJ'), ('revenues', 'NNS'), ('of', 'IN'), ('nearly', 'RB'), ('$', '$'), ('101', 'CD'), ('billion', 'CD'), ('in', 'IN'), ('2000', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print nltk.pos_tag( nltk.word_tokenize(txt_example, language='english') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comentario:__ Pruebe la última instrucción convirtiendo previamente el texto a minúscula y revise la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Que sucede si ensamblan sustantivos consecutivos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enron corporation', 'nyse ticker symbol ene', 'energy company', 'houston', 'texas', 'bankruptcy', 'enron', ']', 'world\\xe2\\x80\\x99s', 'electricity', 'gas', 'pulp', 'paper', 'communications companies', 'revenues']\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag( nltk.word_tokenize(txt_example, language='english') )\n",
    "phrases, phrase = [], \"\"\n",
    "for (word, tag) in tagged:\n",
    "    if tag[:2] == \"NN\":\n",
    "        if phrase == \"\":\n",
    "            phrase = word\n",
    "        else:\n",
    "            phrase += \" \" + word\n",
    "    elif phrase != \"\":\n",
    "        phrases.append(phrase.lower())\n",
    "        phrase = \"\"\n",
    "print phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De documentos a términos y finalmente a vectores en $\\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Se debe primero generar un vocabulario $\\mathcal{V}$ con todos los términos considerados como descriptores de cada documento\n",
    "* Identificar para cada documento la cantidad de ocurrencias a partir de los términos del vocabulario\n",
    "* Construir un vector de largo $|\\mathcal{V}|$ que represente a cada documento usando las ocurrencias de cada término en él"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de una colección de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Content': u'You forgot the smiley-face.  I cant believe this is what they turn out at Berkeley.  Tell me youre an aberration.', u'Label': u'2'}\n"
     ]
    }
   ],
   "source": [
    "#lines = csv.reader(open(\"coleccion.csv\"), delimiter=',', quotechar='\"')\n",
    "lines = csv.reader(open(\"20ng_labeled.csv\"), delimiter=';', quotechar=\"'\")\n",
    "\n",
    "header = []\n",
    "papers = []\n",
    "\n",
    "for row in lines:\n",
    "    line = [unicode(cell, 'utf-8') for cell in row]\n",
    "    if not header:\n",
    "        header = line\n",
    "        continue\n",
    "    papers.append(dict(zip(header, line)))\n",
    "\n",
    "print dict(zip(header, line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender que hace la última línea, ver el ejemplo a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd'}\n"
     ]
    }
   ],
   "source": [
    "print zip([1,2,3,4],['a','b','c','d'])\n",
    "print dict(zip([1,2,3,4],['a','b','c','d']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver por ejemplo como queda estructurado uno de los documentos de la colección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Content', u'Label']\n"
     ]
    }
   ],
   "source": [
    "print papers[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de características mediante eliminación de Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Volviendo a la colección original)__ Ahora se extraeran los términos (palabras) que componen cada _abstract_ usando la función `word_tokenize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "collection_words = {}\n",
    "\n",
    "for paper in papers:\n",
    "    paper_words = nltk.word_tokenize(paper['Content'].lower())\n",
    "    words = {}    \n",
    "    for w in paper_words:\n",
    "        if w not in stopwords and len(w) > 2:\n",
    "            words[w] = words.get(w, 0) + 1    \n",
    "    paper.update({'words': words})\n",
    "    for term in words:\n",
    "        collection_words[term] = collection_words.get(term, 0) + 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta:__ ¿Como realizar esto usando expresiones regulares?\n",
    "\n",
    "__Pregunta:__ ¿Que contiene el diccionario asociado a la llave `words` para cada paper?\n",
    "\n",
    "__Pregunta:__ ¿Que contiene el diccionario `collection_words`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "V = [w for w in collection_words.keys()]\n",
    "# Se crea la matriz de documentos vs terminos\n",
    "N = len(papers)\n",
    "d = len(V)\n",
    "data = sp.lil_matrix((N, d))\n",
    "labels = np.zeros(N,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se generan los vectores para los documentos (uno por cada fila de la matriz). Este paso tarda un tiempo dependiendo de la cantidad y tamaño de los documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for docid in range( len(papers) ):\n",
    "    paper = papers[docid]\n",
    "    labels[docid] = paper['Label']\n",
    "    for w in paper['words']:\n",
    "        data[docid, V.index(w)] = paper['words'][w] * np.log(N/collection_words[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usos posteriores quizás sea conveniente serializar la matriz ya construída, almacenarla en el disco y así acelerar su carga futura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`\n",
    "import pickle\n",
    "pickle.dump( data, open( \"doc_term_matrix.p\", \"wb\" ) )\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para recuperar la matriz a partir del archivo serializado se debe ejecutar:\n",
    "\n",
    "`data = pickle.load( open( \"doc_term_matrix.p\", \"rb\" ) )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte II - Análisis Automático sobre colecciones documentales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Primero__ una versión alternativa ~~y menos controlada~~ para la construcción del vocabulario y de la matriz de documentos vs términos. Para más detalle revisar el enlace en [Scikit-learn](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se contabilizaron 2257 con un vocabulario de 28865 términos.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories=['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med'], remove=('headers', 'footers', 'quotes'))\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(newsgroups_train.data)\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
    "print \"Se contabilizaron\",X_train_tfidf.shape[0],\"con un vocabulario de\",X_train_tfidf.shape[1],\"términos.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando un clasificador Naive Bayes Multinomial y SVM en Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del clasificador NBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_sk = MultinomialNB().fit(X_train_tfidf, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de la SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_clf_sk = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42).fit(X_train_tfidf, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando el dataset de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories=['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med'], remove=('headers', 'footers', 'quotes'))\n",
    "X_new_counts = count_vect.transform(newsgroups_test.data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted_clf_sk = clf_sk.predict(X_new_tfidf)\n",
    "predicted_svm_clf_sk = svm_clf_sk.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño de ambos clasificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Precision] NBM:0.6691 SVM:0.7956\n"
     ]
    }
   ],
   "source": [
    "print \"[Precision] NBM:{0:.4f} SVM:{1:.4f}\".format(np.mean(predicted_clf_sk == newsgroups_test.target),np.mean(predicted_svm_clf_sk == newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando dataset de prueba manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el mismo procedimiento anterior (desde cargar el archivo CSV), usando el vocabulario ya construído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Content': u'Yes, it looks like very good indeed.   Nope.', u'Label': u'1'}\n"
     ]
    }
   ],
   "source": [
    "lines = csv.reader(open(\"20ng_labeled_test.csv\"), delimiter=';', quotechar=\"'\")\n",
    "\n",
    "header = []\n",
    "papers_test = []\n",
    "\n",
    "for row in lines:\n",
    "    line = [unicode(cell, 'utf-8') for cell in row]\n",
    "    if not header:\n",
    "        header = line\n",
    "        continue\n",
    "    papers_test.append(dict(zip(header, line)))\n",
    "\n",
    "print dict(zip(header, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for paper in papers_test:\n",
    "    paper_words = nltk.word_tokenize(paper['Content'].lower())\n",
    "    words = {}    \n",
    "    for w in paper_words:\n",
    "        if w in V and w not in stopwords and len(w) > 2:\n",
    "            words[w] = words.get(w, 0) + 1    \n",
    "    paper.update({'words': words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_test = len(papers_test)\n",
    "data_test = sp.lil_matrix((N_test, d))\n",
    "labels_test = np.zeros(N_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for docid in range( len(papers_test) ):\n",
    "    paper = papers_test[docid]\n",
    "    labels_test[docid] = paper['Label']\n",
    "    for w in paper['words']:\n",
    "        data_test[docid, V.index(w)] = paper['words'][w] * np.log(N/collection_words[w])\n",
    "\n",
    "pickle.dump( data_test, open( \"doc_term_matrix_test.p\", \"wb\" ) )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando un clasificador Naive Bayes Multinomial y SVM entrenados manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_jz = MultinomialNB().fit(data, labels)\n",
    "predicted_clf_jz = clf_jz.predict(data_test)\n",
    "\n",
    "\n",
    "svm_clf_jz = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42).fit(data, labels)\n",
    "predicted_svm_clf_jz = svm_clf_jz.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño de ambos clasificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Precision] NBM:0.8336 SVM:0.7583\n"
     ]
    }
   ],
   "source": [
    "print \"[Precision] NBM:{0:.4f} SVM:{1:.4f}\".format(np.mean(predicted_clf_jz == labels_test), np.mean(predicted_svm_clf_jz == labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la descomposición SVD sobre la matriz de documentos vs términos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "U,S,Vt = svds(data, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:(2257, 300) S:(300, 300) Vt:(300, 33084)\n"
     ]
    }
   ],
   "source": [
    "print \"U:{0} S:{1} Vt:{2}\".format( U.shape,np.diag(S).shape,Vt.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import svd as SVDn\n",
    "Up,Sp,Vp = SVDn(data.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproximación de rango $k$ de la matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "rk_data = np.dot(U[:,:k], np.diag(S)[:k,:k]).dot(Vt[:k,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "print rk_data.shape\n",
    "#norm(data.todense()-rk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "## Librerías (API)\n",
    "\n",
    "[The Natural Language Toolkit](http://www.nltk.org/)\n",
    "\n",
    "[Machine Learning in Python](http://scikit-learn.org/)\n",
    "\n",
    "## Libros\n",
    "\n",
    "[Introduction to Information Retrieval (_gratis_)](http://nlp.stanford.edu/IR-book/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
